{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](6.ResNet架構圖.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.version as v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels,out_channels,stride):\n",
    "\n",
    "    return nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, shortcut=None):\n",
    "        super (ResidualBlock,self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "                    conv3x3(in_channels,out_channels,stride),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(),\n",
    "                    conv3x3(in_channels,out_channels,stride),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.right = shortcut   #根據情況是否做出增維或是縮小shape\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.left(x)\n",
    "        if self.right:\n",
    "            residual =self.right(x)\n",
    "        out+=residual   # f(x)+x\n",
    "        out = nn.ReLU(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, stride=1, shortcut=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, 3, stride, 1, bias=False), # bias=False是因為bias再BN中已經有了，如果stride=2則shape會變成一半\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channel, out_channel, 3, 1, 1, bias=False), # shape前後仍然一漾\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "        )\n",
    "        \n",
    "        self.right = shortcut #根據情況是否做出增維或是縮小shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        residual = x if self.right is None else self.right(x)\n",
    "        out = out + residual\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = t.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualBlock(\n",
       "  (left): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResidualBlock(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, 2, 3, bias=False), #為了使shape變一半，stride必須是2，在固定kernel=7下由公式推得padding=3\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(3, 2, 1) , #為了使shape變一半，stride必須是2，在固定kernel=3下由公式推得padding=1\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.layer1 = self._make_layer(64, 64, 3)\n",
    "        self.layer2 = self._make_layer(64, 128, 4, stride=2) # 對照架構圖，第二段後每次都會將shape再度縮小一半\n",
    "        self.layer3 = self._make_layer(128, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 3, stride=2)\n",
    "        \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, in_channel, out_channel, block_num, stride=1):\n",
    "        \n",
    "        # shortcut的部份必須和該block最後一層維度相同，所以這裡做1d conv增加維度\n",
    "        # 並且根據有沒有縮小shape(stride=2)做相同的動作\n",
    "        shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel,  1, stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "        )\n",
    "        \n",
    "        layers = []\n",
    "        # 第一次的ResidualBlock可能會縮小shape(根據stride)，所以要獨立出來做\n",
    "        layers.append(ResidualBlock(in_channel, out_channel, stride, shortcut)) \n",
    "        \n",
    "        #注意這邊都是第二次以後的ResidualBlock，所以不會有維度或大小不同的問題，參數跟shortcut都不用做\n",
    "        for i in range(1, block_num):\n",
    "            layers.append(ResidualBlock(out_channel, out_channel))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print('origin:', x.shape) # (batch, channel, w, h)\n",
    "        \n",
    "        x = self.pre_layer(x)\n",
    "        print('pre_layer:', x.shape) # (batch, channel, w, h) -> # (batch, 64, w/4, h/4)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print('layer1:', x.shape) # (batch, 64, w/4, h/4) -> (batch, 64, w/4, h/4)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        print('layer2:', x.shape) # (batch, 64, w/4, h/4) -> (batch, 128, w/8, h/8)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        print('layer3:', x.shape) # (batch, 128, w/8, h/8) -> (batch, 256, w/16, h/16)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        print('layer4:', x.shape) # (batch, 256, w/16, h/16) -> (batch, 512, w/32, h/32)\n",
    "        \n",
    "        x = F.avg_pool2d(x, x.shape[3]) \n",
    "        print('avg_pool:', x.shape) # (batch, 512, w/32, h/32) -> (batch, 512, 1, 1)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # (batch, 512, 1, 1) -> (batch, 512 * 1 * 1)\n",
    "        print('flatten:', x.shape)\n",
    "        \n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1(nn.Module):\n",
    "    def __init__(self,num_classes=1000):\n",
    "        super (ResNet1,self).__init__()\n",
    "        \n",
    "        self.pre_layer = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=3,out_channels=64,kernel_size=7,stride=2,padding=3), #64/2 公式推導\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=3,stride=2,padding=1)#pool/2 公式推導\n",
    "        )\n",
    "        \n",
    "        self.layer1 = self.make_layer(64,64,block_num=3)\n",
    "        self.layer2 = self.make_layer(64,128,block_num=4,stride=2)\n",
    "        self.layer3 = self.make_layer(128,256,block_num=6,stride=2)\n",
    "        self.layer4 = self.make_layer(256,512,block_num=3,stride=2)\n",
    "        \n",
    "        self.fc = nn.Linear(512,num_classes)\n",
    "       \n",
    "    \n",
    "    \n",
    "    def make_layer(self, in_channels, out_channels, block_num, stride=1):\n",
    "        \n",
    "        shortcut = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels,out_channels,1,stride,bias=False),\n",
    "                    nn.BatchNorm2d(out_channels) \n",
    "        )\n",
    "\n",
    "        \n",
    "        layer=[]\n",
    "        layer.append(ResidualBlock(in_channels,out_channels,stride,shortcut))\n",
    "        for layerin in range (1,block_num):\n",
    "            layer.append(ResidualBlock(out_channels,out_channels))\n",
    "    \n",
    "        return nn.Sequential(*layer)\n",
    "    \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('origin:', x.shape) # (batch, channel, w, h)\n",
    "        \n",
    "        x = self.pre_layer(x)\n",
    "        print('pre_layer:', x.shape) # (batch, channel, w, h) -> # (batch, 64, w/4, h/4)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print('layer1:', x.shape) # (batch, 64, w/4, h/4) -> (batch, 64, w/4, h/4)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        print('layer2:', x.shape) # (batch, 64, w/4, h/4) -> (batch, 128, w/8, h/8)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        print('layer3:', x.shape) # (batch, 128, w/8, h/8) -> (batch, 256, w/16, h/16)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        print('layer4:', x.shape) # (batch, 256, w/16, h/16) -> (batch, 512, w/32, h/32)\n",
    "        \n",
    "        x = F.avg_pool2d(x, x.shape[3]) \n",
    "        print('avg_pool:', x.shape) # (batch, 512, w/32, h/32) -> (batch, 512, 1, 1)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # (batch, 512, 1, 1) -> (batch, 512 * 1 * 1)\n",
    "        print('flatten:', x.shape)\n",
    "        \n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet1(\n",
       "  (pre_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (right): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (right): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (right): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (right): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet34 = ResNet1()\n",
    "resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: torch.Size([1, 3, 228, 228])\n",
      "pre_layer: torch.Size([1, 64, 57, 57])\n",
      "layer1: torch.Size([1, 64, 57, 57])\n",
      "layer2: torch.Size([1, 128, 29, 29])\n",
      "layer3: torch.Size([1, 256, 15, 15])\n",
      "layer4: torch.Size([1, 512, 8, 8])\n",
      "avg_pool: torch.Size([1, 512, 1, 1])\n",
      "flatten: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "test_input = t.randn(1, 3, 228, 228)\n",
    "test_out = resnet34(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def make_dot(var, params):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "    \n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "    \n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    param_map = {id(v): k for k, v in params.items()}\n",
    "    print(param_map)\n",
    "    \n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "    \n",
    "    def size_to_str(size):\n",
    "        return '('+(', ').join(['%d'% v for v in size])+')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if t.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                node_name = '%s\\n %s' % (param_map.get(id(u)), size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for time in var.saved_tensors:\n",
    "                    dot.edge(str(id(time)), str(id(var)))\n",
    "                    add_nodes(time)\n",
    "    add_nodes(var.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: torch.Size([1, 3, 228, 228])\n",
      "pre_layer: torch.Size([1, 64, 57, 57])\n",
      "layer1: torch.Size([1, 64, 57, 57])\n",
      "layer2: torch.Size([1, 128, 29, 29])\n",
      "layer3: torch.Size([1, 256, 15, 15])\n",
      "layer4: torch.Size([1, 512, 8, 8])\n",
      "avg_pool: torch.Size([1, 512, 1, 1])\n",
      "flatten: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "test_input = t.randn(1, 3, 228, 228)\n",
    "test_out = resnet34(Variable(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2221531246024: 'pre_layer.0.weight', 2221531245880: 'pre_layer.0.bias', 2221531245808: 'pre_layer.1.weight', 2221530611576: 'pre_layer.1.bias', 2221530611216: 'pre_layer.1.running_mean', 2221602226464: 'pre_layer.1.running_var', 2221602226248: 'pre_layer.1.num_batches_tracked', 2221602226680: 'layer1.0.left.0.weight', 2221602227616: 'layer1.0.left.1.weight', 2221602227760: 'layer1.0.left.1.bias', 2221602227904: 'layer1.0.left.1.running_mean', 2221602227976: 'layer1.0.left.1.running_var', 2221602228048: 'layer1.0.left.1.num_batches_tracked', 2221602228264: 'layer1.0.left.3.weight', 2221602227400: 'layer1.0.left.4.weight', 2221602228552: 'layer1.0.left.4.bias', 2221602228768: 'layer1.0.left.4.running_mean', 2221602228840: 'layer1.0.left.4.running_var', 2221602228912: 'layer1.0.left.4.num_batches_tracked', 2221602228984: 'layer1.0.right.0.weight', 2221602229488: 'layer1.0.right.1.weight', 2221602229272: 'layer1.0.right.1.bias', 2221602229848: 'layer1.0.right.1.running_mean', 2221602230064: 'layer1.0.right.1.running_var', 2221602230208: 'layer1.0.right.1.num_batches_tracked', 2221602229776: 'layer1.1.left.0.weight', 2221602251328: 'layer1.1.left.1.weight', 2221602251256: 'layer1.1.left.1.bias', 2221602251040: 'layer1.1.left.1.running_mean', 2221602251112: 'layer1.1.left.1.running_var', 2221602250968: 'layer1.1.left.1.num_batches_tracked', 2221602251832: 'layer1.1.left.3.weight', 2221602253848: 'layer1.1.left.4.weight', 2221602253920: 'layer1.1.left.4.bias', 2221602254136: 'layer1.1.left.4.running_mean', 2221602254280: 'layer1.1.left.4.running_var', 2221602254496: 'layer1.1.left.4.num_batches_tracked', 2221602254424: 'layer1.2.left.0.weight', 2221602254208: 'layer1.2.left.1.weight', 2221602253488: 'layer1.2.left.1.bias', 2221602253632: 'layer1.2.left.1.running_mean', 2221602253416: 'layer1.2.left.1.running_var', 2221602253344: 'layer1.2.left.1.num_batches_tracked', 2221602253272: 'layer1.2.left.3.weight', 2221602252768: 'layer1.2.left.4.weight', 2221602252480: 'layer1.2.left.4.bias', 2221602253128: 'layer1.2.left.4.running_mean', 2221602253056: 'layer1.2.left.4.running_var', 2221602252984: 'layer1.2.left.4.num_batches_tracked', 2221602252120: 'layer2.0.left.0.weight', 2221602278496: 'layer2.0.left.1.weight', 2221602276336: 'layer2.0.left.1.bias', 2221602275616: 'layer2.0.left.1.running_mean', 2221602275760: 'layer2.0.left.1.running_var', 2221602275904: 'layer2.0.left.1.num_batches_tracked', 2221602275544: 'layer2.0.left.3.weight', 2221602276192: 'layer2.0.left.4.weight', 2221602276120: 'layer2.0.left.4.bias', 2221602276840: 'layer2.0.left.4.running_mean', 2221602276480: 'layer2.0.left.4.running_var', 2221602276624: 'layer2.0.left.4.num_batches_tracked', 2221602276408: 'layer2.0.right.0.weight', 2221602289200: 'layer2.0.right.1.weight', 2221602289416: 'layer2.0.right.1.bias', 2221602289128: 'layer2.0.right.1.running_mean', 2221602289560: 'layer2.0.right.1.running_var', 2221602289848: 'layer2.0.right.1.num_batches_tracked', 2221602289488: 'layer2.1.left.0.weight', 2221602287976: 'layer2.1.left.1.weight', 2221602287904: 'layer2.1.left.1.bias', 2221602287760: 'layer2.1.left.1.running_mean', 2221602288840: 'layer2.1.left.1.running_var', 2221602288192: 'layer2.1.left.1.num_batches_tracked', 2221602288336: 'layer2.1.left.3.weight', 2221602288408: 'layer2.1.left.4.weight', 2221602288624: 'layer2.1.left.4.bias', 2221602288048: 'layer2.1.left.4.running_mean', 2221602290280: 'layer2.1.left.4.running_var', 2221602290136: 'layer2.1.left.4.num_batches_tracked', 2221602290640: 'layer2.2.left.0.weight', 2221602291648: 'layer2.2.left.1.weight', 2221602291504: 'layer2.2.left.1.bias', 2221602291360: 'layer2.2.left.1.running_mean', 2221602291072: 'layer2.2.left.1.running_var', 2221602291216: 'layer2.2.left.1.num_batches_tracked', 2221602290928: 'layer2.2.left.3.weight', 2221602234512: 'layer2.2.left.4.weight', 2221602335840: 'layer2.2.left.4.bias', 2221602335984: 'layer2.2.left.4.running_mean', 2221602336056: 'layer2.2.left.4.running_var', 2221602336128: 'layer2.2.left.4.num_batches_tracked', 2221602336416: 'layer2.3.left.0.weight', 2221602336632: 'layer2.3.left.1.weight', 2221602394184: 'layer2.3.left.1.bias', 2221602394328: 'layer2.3.left.1.running_mean', 2221602394400: 'layer2.3.left.1.running_var', 2221602394472: 'layer2.3.left.1.num_batches_tracked', 2221602394688: 'layer2.3.left.3.weight', 2221602394904: 'layer2.3.left.4.weight', 2221602395048: 'layer2.3.left.4.bias', 2221602395192: 'layer2.3.left.4.running_mean', 2221602395264: 'layer2.3.left.4.running_var', 2221602395336: 'layer2.3.left.4.num_batches_tracked', 2221602395696: 'layer3.0.left.0.weight', 2221602395912: 'layer3.0.left.1.weight', 2221602396056: 'layer3.0.left.1.bias', 2221602396200: 'layer3.0.left.1.running_mean', 2221602396272: 'layer3.0.left.1.running_var', 2221602396344: 'layer3.0.left.1.num_batches_tracked', 2221602396560: 'layer3.0.left.3.weight', 2221602396776: 'layer3.0.left.4.weight', 2221602396920: 'layer3.0.left.4.bias', 2221602397064: 'layer3.0.left.4.running_mean', 2221602397136: 'layer3.0.left.4.running_var', 2221602397208: 'layer3.0.left.4.num_batches_tracked', 2221602397496: 'layer3.0.right.0.weight', 2221602397784: 'layer3.0.right.1.weight', 2221602397928: 'layer3.0.right.1.bias', 2221602398072: 'layer3.0.right.1.running_mean', 2221602398144: 'layer3.0.right.1.running_var', 2221602406472: 'layer3.0.right.1.num_batches_tracked', 2221602406760: 'layer3.1.left.0.weight', 2221602406976: 'layer3.1.left.1.weight', 2221602407120: 'layer3.1.left.1.bias', 2221602407264: 'layer3.1.left.1.running_mean', 2221602407336: 'layer3.1.left.1.running_var', 2221602407408: 'layer3.1.left.1.num_batches_tracked', 2221602407624: 'layer3.1.left.3.weight', 2221602407840: 'layer3.1.left.4.weight', 2221602407984: 'layer3.1.left.4.bias', 2221602408128: 'layer3.1.left.4.running_mean', 2221602408200: 'layer3.1.left.4.running_var', 2221602408272: 'layer3.1.left.4.num_batches_tracked', 2221602408560: 'layer3.2.left.0.weight', 2221602408776: 'layer3.2.left.1.weight', 2221602408920: 'layer3.2.left.1.bias', 2221602409064: 'layer3.2.left.1.running_mean', 2221602409136: 'layer3.2.left.1.running_var', 2221602409208: 'layer3.2.left.1.num_batches_tracked', 2221602409424: 'layer3.2.left.3.weight', 2221602409640: 'layer3.2.left.4.weight', 2221602409784: 'layer3.2.left.4.bias', 2221602409928: 'layer3.2.left.4.running_mean', 2221602410000: 'layer3.2.left.4.running_var', 2221602410072: 'layer3.2.left.4.num_batches_tracked', 2221602410360: 'layer3.3.left.0.weight', 2221602414736: 'layer3.3.left.1.weight', 2221602414880: 'layer3.3.left.1.bias', 2221602415024: 'layer3.3.left.1.running_mean', 2221602415096: 'layer3.3.left.1.running_var', 2221602415168: 'layer3.3.left.1.num_batches_tracked', 2221602415384: 'layer3.3.left.3.weight', 2221602415600: 'layer3.3.left.4.weight', 2221602415744: 'layer3.3.left.4.bias', 2221602415888: 'layer3.3.left.4.running_mean', 2221602415960: 'layer3.3.left.4.running_var', 2221602416032: 'layer3.3.left.4.num_batches_tracked', 2221602416320: 'layer3.4.left.0.weight', 2221602416536: 'layer3.4.left.1.weight', 2221602416680: 'layer3.4.left.1.bias', 2221602416824: 'layer3.4.left.1.running_mean', 2221602416896: 'layer3.4.left.1.running_var', 2221602416968: 'layer3.4.left.1.num_batches_tracked', 2221602417184: 'layer3.4.left.3.weight', 2221602417400: 'layer3.4.left.4.weight', 2221602417544: 'layer3.4.left.4.bias', 2221602417688: 'layer3.4.left.4.running_mean', 2221602417760: 'layer3.4.left.4.running_var', 2221602417832: 'layer3.4.left.4.num_batches_tracked', 2221602418120: 'layer3.5.left.0.weight', 2221602418336: 'layer3.5.left.1.weight', 2221602418480: 'layer3.5.left.1.bias', 2221602418624: 'layer3.5.left.1.running_mean', 2221602422856: 'layer3.5.left.1.running_var', 2221602422928: 'layer3.5.left.1.num_batches_tracked', 2221602423144: 'layer3.5.left.3.weight', 2221602423360: 'layer3.5.left.4.weight', 2221602423504: 'layer3.5.left.4.bias', 2221602423648: 'layer3.5.left.4.running_mean', 2221602423720: 'layer3.5.left.4.running_var', 2221602423792: 'layer3.5.left.4.num_batches_tracked', 2221602424152: 'layer4.0.left.0.weight', 2221602424368: 'layer4.0.left.1.weight', 2221602424512: 'layer4.0.left.1.bias', 2221602424656: 'layer4.0.left.1.running_mean', 2221602424728: 'layer4.0.left.1.running_var', 2221602424800: 'layer4.0.left.1.num_batches_tracked', 2221602425016: 'layer4.0.left.3.weight', 2221602425232: 'layer4.0.left.4.weight', 2221602425376: 'layer4.0.left.4.bias', 2221602425520: 'layer4.0.left.4.running_mean', 2221602425592: 'layer4.0.left.4.running_var', 2221602425664: 'layer4.0.left.4.num_batches_tracked', 2221602425952: 'layer4.0.right.0.weight', 2221602426240: 'layer4.0.right.1.weight', 2221602426384: 'layer4.0.right.1.bias', 2221602426528: 'layer4.0.right.1.running_mean', 2221602426600: 'layer4.0.right.1.running_var', 2221602426672: 'layer4.0.right.1.num_batches_tracked', 2221602431120: 'layer4.1.left.0.weight', 2221602431336: 'layer4.1.left.1.weight', 2221602431480: 'layer4.1.left.1.bias', 2221602431624: 'layer4.1.left.1.running_mean', 2221602431696: 'layer4.1.left.1.running_var', 2221602431768: 'layer4.1.left.1.num_batches_tracked', 2221602431984: 'layer4.1.left.3.weight', 2221602432200: 'layer4.1.left.4.weight', 2221602432344: 'layer4.1.left.4.bias', 2221602432488: 'layer4.1.left.4.running_mean', 2221602432560: 'layer4.1.left.4.running_var', 2221602432632: 'layer4.1.left.4.num_batches_tracked', 2221602432920: 'layer4.2.left.0.weight', 2221602433136: 'layer4.2.left.1.weight', 2221602433280: 'layer4.2.left.1.bias', 2221602433424: 'layer4.2.left.1.running_mean', 2221602433496: 'layer4.2.left.1.running_var', 2221602433568: 'layer4.2.left.1.num_batches_tracked', 2221602433784: 'layer4.2.left.3.weight', 2221602434000: 'layer4.2.left.4.weight', 2221602434144: 'layer4.2.left.4.bias', 2221602434288: 'layer4.2.left.4.running_mean', 2221602434360: 'layer4.2.left.4.running_var', 2221602434432: 'layer4.2.left.4.num_batches_tracked', 2221602434504: 'fc.weight', 2221602434576: 'fc.bias'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = make_dot(test_out,resnet34.state_dict())\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam(a, b=None):\n",
    "    if b is None:\n",
    "        print(2)\n",
    "    elif b:\n",
    "        print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a=None\n",
    "spam(1,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
