{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.version as v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels,out_channels,stride):\n",
    "\n",
    "    return nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, shortcut=None):\n",
    "        super (ResidualBlock,self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "                    conv3x3(in_channels,out_channels,stride),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(),\n",
    "                    conv3x3(in_channels,out_channels,stride),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.right = shortcut   #根據情況是否做出增維或是縮小shape\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.left(x)\n",
    "        if self.right:\n",
    "            residual =self.right(x)\n",
    "        out+=residual   # f(x)+x\n",
    "        out = nn.ReLU(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, stride=1, shortcut=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, 3, stride, 1, bias=False), # bias=False是因為bias再BN中已經有了，如果stride=2則shape會變成一半\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channel, out_channel, 3, 1, 1, bias=False), # shape前後仍然一漾\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "        )\n",
    "        \n",
    "        self.right = shortcut #根據情況是否做出增維或是縮小shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        residual = x if self.right is None else self.right(x)\n",
    "        out = out + residual\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = t.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualBlock(\n",
       "  (left): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResidualBlock(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, 2, 3, bias=False), #為了使shape變一半，stride必須是2，在固定kernel=7下由公式推得padding=3\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2, 1) , #為了使shape變一半，stride必須是2，在固定kernel=3下由公式推得padding=1\n",
    "        )\n",
    "        \n",
    "        self.layer1 = self._make_layer(64, 64, 3)\n",
    "        self.layer2 = self._make_layer(64, 128, 4, stride=2) # 對照架構圖，第二段後每次都會將shape再度縮小一半\n",
    "        self.layer3 = self._make_layer(128, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 3, stride=2)\n",
    "        \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, in_channel, out_channel, block_num, stride=1):\n",
    "        \n",
    "        # shortcut的部份必須和該block最後一層維度相同，所以這裡做1d conv增加維度\n",
    "        # 並且根據有沒有縮小shape(stride=2)做相同的動作\n",
    "        shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel,  1, stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "        )\n",
    "        \n",
    "        layers = []\n",
    "        # 第一次的ResidualBlock可能會縮小shape(根據stride)，所以要獨立出來做\n",
    "        layers.append(ResidualBlock(in_channel, out_channel, stride, shortcut)) \n",
    "        \n",
    "        #注意這邊都是第二次以後的ResidualBlock，所以不會有維度或大小不同的問題，參數跟shortcut都不用做\n",
    "        for i in range(1, block_num):\n",
    "            layers.append(ResidualBlock(out_channel, out_channel))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print('origin:', x.shape) # (batch, channel, w, h)\n",
    "        \n",
    "        x = self.pre_layer(x)\n",
    "        print('pre_layer:', x.shape) # (batch, channel, w, h) -> # (batch, 64, w/4, h/4)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print('layer1:', x.shape) # (batch, 64, w/4, h/4) -> (batch, 64, w/4, h/4)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        print('layer2:', x.shape) # (batch, 64, w/4, h/4) -> (batch, 128, w/8, h/8)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        print('layer3:', x.shape) # (batch, 128, w/8, h/8) -> (batch, 256, w/16, h/16)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        print('layer4:', x.shape) # (batch, 256, w/16, h/16) -> (batch, 512, w/32, h/32)\n",
    "        \n",
    "        x = F.avg_pool2d(x, x.shape[3]) \n",
    "        print('avg_pool:', x.shape) # (batch, 512, w/32, h/32) -> (batch, 512, 1, 1)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # (batch, 512, 1, 1) -> (batch, 512 * 1 * 1)\n",
    "        print('flatten:', x.shape)\n",
    "        \n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1(nn.Module):\n",
    "    def __init__(self,num_classes=1000):\n",
    "        super (ResNet1,self).__init__()\n",
    "        \n",
    "        self.pre_layer = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=3,out_channels=64,kernel_size=7,stride=2,padding=3), #64/2 公式推導\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=3,stride=2,padding=1)#pool/2 公式推導\n",
    "        )\n",
    "        \n",
    "        self.layer1 = self.make_layer(64,64,block_num=3)\n",
    "        self.layer2 = self.make_layer(64,128,block_num=4,stride=2)\n",
    "        self.layer3 = self.make_layer(128,256,block_num=6,stride=2)\n",
    "        self.layer4 = self.make_layer(256,512,block_num=3,stride=2)\n",
    "        \n",
    "        self.fc = nn.Linear(512,num_classes)\n",
    "       \n",
    "    \n",
    "    \n",
    "    def make_layer(self, in_channels, out_channels, block_num, stride=1):\n",
    "        \n",
    "        shortcut = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels,out_channels,1,stride,bias=False),\n",
    "                    nn.BatchNorm2d(out_channels) \n",
    "        )\n",
    "\n",
    "        \n",
    "        layer=[]\n",
    "        layer.append(ResidualBlock(in_channels,out_channels,stride,shortcut))\n",
    "        for layerin in range (1,block_num):\n",
    "            layer.append(ResidualBlock(out_channels,out_channels))\n",
    "    \n",
    "        return nn.Sequential(*layer)\n",
    "    \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('origin:', x.shape) # (batch, channel, w, h)\n",
    "        \n",
    "        x = self.pre_layer(x)\n",
    "        print('pre_layer:', x.shape) # (batch, channel, w, h) -> # (batch, 64, w/4, h/4)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print('layer1:', x.shape) # (batch, 64, w/4, h/4) -> (batch, 64, w/4, h/4)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        print('layer2:', x.shape) # (batch, 64, w/4, h/4) -> (batch, 128, w/8, h/8)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        print('layer3:', x.shape) # (batch, 128, w/8, h/8) -> (batch, 256, w/16, h/16)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        print('layer4:', x.shape) # (batch, 256, w/16, h/16) -> (batch, 512, w/32, h/32)\n",
    "        \n",
    "        x = F.avg_pool2d(x, x.shape[3]) \n",
    "        print('avg_pool:', x.shape) # (batch, 512, w/32, h/32) -> (batch, 512, 1, 1)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # (batch, 512, 1, 1) -> (batch, 512 * 1 * 1)\n",
    "        print('flatten:', x.shape)\n",
    "        \n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet1(\n",
       "  (pre_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (right): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (right): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (right): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (right): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet34 = ResNet1()\n",
    "resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: torch.Size([1, 3, 224, 224])\n",
      "pre_layer: torch.Size([1, 64, 56, 56])\n",
      "layer1: torch.Size([1, 64, 56, 56])\n",
      "layer2: torch.Size([1, 128, 28, 28])\n",
      "layer3: torch.Size([1, 256, 14, 14])\n",
      "layer4: torch.Size([1, 512, 7, 7])\n",
      "avg_pool: torch.Size([1, 512, 1, 1])\n",
      "flatten: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "test_input = t.randn(1, 3, 224, 224)\n",
    "test_out = resnet34(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def make_dot(var, params):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "    \n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "    \n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    param_map = {id(v): k for k, v in params.items()}\n",
    "    print(param_map)\n",
    "    \n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "    \n",
    "    def size_to_str(size):\n",
    "        return '('+(', ').join(['%d'% v for v in size])+')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if t.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                node_name = '%s\\n %s' % (param_map.get(id(u)), size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for time in var.saved_tensors:\n",
    "                    dot.edge(str(id(time)), str(id(var)))\n",
    "                    add_nodes(time)\n",
    "    add_nodes(var.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: torch.Size([1, 3, 224, 224])\n",
      "pre_layer: torch.Size([1, 64, 56, 56])\n",
      "layer1: torch.Size([1, 64, 56, 56])\n",
      "layer2: torch.Size([1, 128, 28, 28])\n",
      "layer3: torch.Size([1, 256, 14, 14])\n",
      "layer4: torch.Size([1, 512, 7, 7])\n",
      "avg_pool: torch.Size([1, 512, 1, 1])\n",
      "flatten: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "test_input = t.randn(1, 3, 224, 224)\n",
    "test_out = resnet34(Variable(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2265397287456: 'pre_layer.0.weight', 2265397286088: 'pre_layer.0.bias', 2265397286880: 'pre_layer.1.weight', 2265397285296: 'pre_layer.1.bias', 2265397287600: 'pre_layer.1.running_mean', 2265397619016: 'pre_layer.1.running_var', 2265397619376: 'pre_layer.1.num_batches_tracked', 2265395132960: 'layer1.0.left.0.weight', 2265395101632: 'layer1.0.left.1.weight', 2265397265464: 'layer1.0.left.1.bias', 2265397268344: 'layer1.0.left.1.running_mean', 2265397265320: 'layer1.0.left.1.running_var', 2265397268056: 'layer1.0.left.1.num_batches_tracked', 2265397267120: 'layer1.0.left.3.weight', 2265396454960: 'layer1.0.left.4.weight', 2265395832152: 'layer1.0.left.4.bias', 2265395833232: 'layer1.0.left.4.running_mean', 2265394884896: 'layer1.0.left.4.running_var', 2265392732848: 'layer1.0.left.4.num_batches_tracked', 2265397576904: 'layer1.0.right.0.weight', 2265397576328: 'layer1.0.right.1.weight', 2265397576544: 'layer1.0.right.1.bias', 2265397576472: 'layer1.0.right.1.running_mean', 2265397576976: 'layer1.0.right.1.running_var', 2265395893736: 'layer1.0.right.1.num_batches_tracked', 2265396339192: 'layer1.1.left.0.weight', 2265396049312: 'layer1.1.left.1.weight', 2265396048664: 'layer1.1.left.1.bias', 2265394790688: 'layer1.1.left.1.running_mean', 2265392264104: 'layer1.1.left.1.running_var', 2265392356016: 'layer1.1.left.1.num_batches_tracked', 2265390503616: 'layer1.1.left.3.weight', 2265390503040: 'layer1.1.left.4.weight', 2265390503688: 'layer1.1.left.4.bias', 2265390502752: 'layer1.1.left.4.running_mean', 2265390503328: 'layer1.1.left.4.running_var', 2265390503184: 'layer1.1.left.4.num_batches_tracked', 2265392491112: 'layer1.2.left.0.weight', 2265397769488: 'layer1.2.left.1.weight', 2265397769560: 'layer1.2.left.1.bias', 2265397768912: 'layer1.2.left.1.running_mean', 2265397768696: 'layer1.2.left.1.running_var', 2265397768408: 'layer1.2.left.1.num_batches_tracked', 2265397770280: 'layer1.2.left.3.weight', 2265397771576: 'layer1.2.left.4.weight', 2265396125056: 'layer1.2.left.4.bias', 2265397146680: 'layer1.2.left.4.running_mean', 2265395912064: 'layer1.2.left.4.running_var', 2265395910696: 'layer1.2.left.4.num_batches_tracked', 2265397596024: 'layer2.0.left.0.weight', 2265397595808: 'layer2.0.left.1.weight', 2265397593000: 'layer2.0.left.1.bias', 2265397594728: 'layer2.0.left.1.running_mean', 2265397595664: 'layer2.0.left.1.running_var', 2265397594872: 'layer2.0.left.1.num_batches_tracked', 2265397595088: 'layer2.0.left.3.weight', 2265395939296: 'layer2.0.left.4.weight', 2265392604288: 'layer2.0.left.4.bias', 2265396763888: 'layer2.0.left.4.running_mean', 2265396761872: 'layer2.0.left.4.running_var', 2265396761296: 'layer2.0.left.4.num_batches_tracked', 2265397792416: 'layer2.0.right.0.weight', 2265396355792: 'layer2.0.right.1.weight', 2265396357880: 'layer2.0.right.1.bias', 2265397672264: 'layer2.0.right.1.running_mean', 2265397672624: 'layer2.0.right.1.running_var', 2265396246064: 'layer2.0.right.1.num_batches_tracked', 2265397741680: 'layer2.1.left.0.weight', 2265397742616: 'layer2.1.left.1.weight', 2265397742040: 'layer2.1.left.1.bias', 2265397741032: 'layer2.1.left.1.running_mean', 2265397741104: 'layer2.1.left.1.running_var', 2265397742472: 'layer2.1.left.1.num_batches_tracked', 2265397740600: 'layer2.1.left.3.weight', 2265397742688: 'layer2.1.left.4.weight', 2265396223432: 'layer2.1.left.4.bias', 2265396288968: 'layer2.1.left.4.running_mean', 2265396287960: 'layer2.1.left.4.running_var', 2265392935632: 'layer2.1.left.4.num_batches_tracked', 2265397383384: 'layer2.2.left.0.weight', 2265397404296: 'layer2.2.left.1.weight', 2265397405088: 'layer2.2.left.1.bias', 2265394427584: 'layer2.2.left.1.running_mean', 2265395058224: 'layer2.2.left.1.running_var', 2265394980544: 'layer2.2.left.1.num_batches_tracked', 2265394814480: 'layer2.2.left.3.weight', 2265397195472: 'layer2.2.left.4.weight', 2265396149200: 'layer2.2.left.4.bias', 2265397251456: 'layer2.2.left.4.running_mean', 2265397252032: 'layer2.2.left.4.running_var', 2265397251816: 'layer2.2.left.4.num_batches_tracked', 2265395344944: 'layer2.3.left.0.weight', 2265397093792: 'layer2.3.left.1.weight', 2265395002536: 'layer2.3.left.1.bias', 2265395435200: 'layer2.3.left.1.running_mean', 2265395395824: 'layer2.3.left.1.running_var', 2265394384400: 'layer2.3.left.1.num_batches_tracked', 2265395315840: 'layer2.3.left.3.weight', 2265391467400: 'layer2.3.left.4.weight', 2265397358096: 'layer2.3.left.4.bias', 2265397355792: 'layer2.3.left.4.running_mean', 2265397355720: 'layer2.3.left.4.running_var', 2265397357520: 'layer2.3.left.4.num_batches_tracked', 2265397356296: 'layer3.0.left.0.weight', 2265391446776: 'layer3.0.left.1.weight', 2265395244920: 'layer3.0.left.1.bias', 2265397341568: 'layer3.0.left.1.running_mean', 2265397341712: 'layer3.0.left.1.running_var', 2265397338832: 'layer3.0.left.1.num_batches_tracked', 2265395224296: 'layer3.0.left.3.weight', 2265391404376: 'layer3.0.left.4.weight', 2265397310016: 'layer3.0.left.4.bias', 2265397311312: 'layer3.0.left.4.running_mean', 2265397310088: 'layer3.0.left.4.running_var', 2265397310376: 'layer3.0.left.4.num_batches_tracked', 2265397525528: 'layer3.0.right.0.weight', 2265397524664: 'layer3.0.right.1.weight', 2265397522504: 'layer3.0.right.1.bias', 2265397523296: 'layer3.0.right.1.running_mean', 2265397839480: 'layer3.0.right.1.running_var', 2265397838040: 'layer3.0.right.1.num_batches_tracked', 2265397838328: 'layer3.1.left.0.weight', 2265397841568: 'layer3.1.left.1.weight', 2265397841856: 'layer3.1.left.1.bias', 2265397839840: 'layer3.1.left.1.running_mean', 2265397838616: 'layer3.1.left.1.running_var', 2265397839552: 'layer3.1.left.1.num_batches_tracked', 2265396479104: 'layer3.1.left.3.weight', 2265396481048: 'layer3.1.left.4.weight', 2265390530632: 'layer3.1.left.4.bias', 2265390531712: 'layer3.1.left.4.running_mean', 2265390532216: 'layer3.1.left.4.running_var', 2265390532288: 'layer3.1.left.4.num_batches_tracked', 2265390532072: 'layer3.2.left.0.weight', 2265390533368: 'layer3.2.left.1.weight', 2265390533512: 'layer3.2.left.1.bias', 2265390533656: 'layer3.2.left.1.running_mean', 2265390533728: 'layer3.2.left.1.running_var', 2265390533800: 'layer3.2.left.1.num_batches_tracked', 2265390534016: 'layer3.2.left.3.weight', 2265390534232: 'layer3.2.left.4.weight', 2265390534448: 'layer3.2.left.4.bias', 2265390533224: 'layer3.2.left.4.running_mean', 2265393267480: 'layer3.2.left.4.running_var', 2265393269568: 'layer3.2.left.4.num_batches_tracked', 2265396499296: 'layer3.3.left.0.weight', 2265396501024: 'layer3.3.left.1.weight', 2265396519128: 'layer3.3.left.1.bias', 2265396521144: 'layer3.3.left.1.running_mean', 2265396519416: 'layer3.3.left.1.running_var', 2265396522008: 'layer3.3.left.1.num_batches_tracked', 2265396522296: 'layer3.3.left.3.weight', 2265397854640: 'layer3.3.left.4.weight', 2265397854856: 'layer3.3.left.4.bias', 2265397855000: 'layer3.3.left.4.running_mean', 2265397854928: 'layer3.3.left.4.running_var', 2265397855216: 'layer3.3.left.4.num_batches_tracked', 2265397855360: 'layer3.4.left.0.weight', 2265397855648: 'layer3.4.left.1.weight', 2265397855720: 'layer3.4.left.1.bias', 2265397855936: 'layer3.4.left.1.running_mean', 2265397856224: 'layer3.4.left.1.running_var', 2265397855576: 'layer3.4.left.1.num_batches_tracked', 2265397856440: 'layer3.4.left.3.weight', 2265397856656: 'layer3.4.left.4.weight', 2265397856728: 'layer3.4.left.4.bias', 2265397856584: 'layer3.4.left.4.running_mean', 2265397856872: 'layer3.4.left.4.running_var', 2265397857088: 'layer3.4.left.4.num_batches_tracked', 2265397856944: 'layer3.5.left.0.weight', 2265397857736: 'layer3.5.left.1.weight', 2265397858096: 'layer3.5.left.1.bias', 2265397857880: 'layer3.5.left.1.running_mean', 2265397858240: 'layer3.5.left.1.running_var', 2265397858168: 'layer3.5.left.1.num_batches_tracked', 2265396549744: 'layer3.5.left.3.weight', 2265396549168: 'layer3.5.left.4.weight', 2265396551184: 'layer3.5.left.4.bias', 2265396569144: 'layer3.5.left.4.running_mean', 2265396570152: 'layer3.5.left.4.running_var', 2265396568568: 'layer3.5.left.4.num_batches_tracked', 2265396596808: 'layer4.0.left.0.weight', 2265396597240: 'layer4.0.left.1.weight', 2265396598176: 'layer4.0.left.1.bias', 2265396599112: 'layer4.0.left.1.running_mean', 2265396600192: 'layer4.0.left.1.running_var', 2265397888632: 'layer4.0.left.1.num_batches_tracked', 2265397887264: 'layer4.0.left.3.weight', 2265397887336: 'layer4.0.left.4.weight', 2265397887480: 'layer4.0.left.4.bias', 2265397887912: 'layer4.0.left.4.running_mean', 2265397888128: 'layer4.0.left.4.running_var', 2265397888200: 'layer4.0.left.4.num_batches_tracked', 2265397888272: 'layer4.0.right.0.weight', 2265397888704: 'layer4.0.right.1.weight', 2265397888992: 'layer4.0.right.1.bias', 2265397888920: 'layer4.0.right.1.running_mean', 2265397889208: 'layer4.0.right.1.running_var', 2265397889280: 'layer4.0.right.1.num_batches_tracked', 2265397889640: 'layer4.1.left.0.weight', 2265397889064: 'layer4.1.left.1.weight', 2265397889856: 'layer4.1.left.1.bias', 2265397890144: 'layer4.1.left.1.running_mean', 2265397890216: 'layer4.1.left.1.running_var', 2265397890360: 'layer4.1.left.1.num_batches_tracked', 2265397890000: 'layer4.1.left.3.weight', 2265397890720: 'layer4.1.left.4.weight', 2265397890288: 'layer4.1.left.4.bias', 2265397890864: 'layer4.1.left.4.running_mean', 2265396618296: 'layer4.1.left.4.running_var', 2265396619376: 'layer4.1.left.4.num_batches_tracked', 2265396619808: 'layer4.2.left.0.weight', 2265396643088: 'layer4.2.left.1.weight', 2265396642296: 'layer4.2.left.1.bias', 2265396643376: 'layer4.2.left.1.running_mean', 2265396644528: 'layer4.2.left.1.running_var', 2265396645536: 'layer4.2.left.1.num_batches_tracked', 2265397907672: 'layer4.2.left.3.weight', 2265397908176: 'layer4.2.left.4.weight', 2265397908392: 'layer4.2.left.4.bias', 2265397908536: 'layer4.2.left.4.running_mean', 2265397908320: 'layer4.2.left.4.running_var', 2265397908464: 'layer4.2.left.4.num_batches_tracked', 2265397908248: 'fc.weight', 2265397908680: 'fc.bias'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = make_dot(test_out,resnet34.state_dict())\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam(a, b=None):\n",
    "    if b is None:\n",
    "        print(2)\n",
    "    elif b:\n",
    "        print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=None\n",
    "spam(1,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
